---
title: "Mysql面经"
description: "mysql面经"
keywords: "mysql面经"

date: 2024-02-28T09:56:32+08:00
lastmod: 2024-02-28T09:56:32+08:00

categories:
  - 面试
tags:
  - mysql
  - 面经

# 原文作者
# Post's origin author name
author: zzzi
# 开启数学公式渲染，可选值： mathjax, katex
# Support Math Formulas render, options: mathjax, katex
math: mathjax
# 原文链接
# Post's origin link URL
#link:
# 图片链接，用在open graph和twitter卡片上
# Image source link that will use in open graph and twitter card
#imgs:
# 在首页展开内容
# Expand content on the home page
#expand: true
# 外部链接地址，访问时直接跳转
# It's means that will redirecting to external links
#extlink:
# 在当前页面关闭评论功能
# Disabled comment plugins in this post
#comment:
#  enable: false
# 关闭文章目录功能
# Disable table of content
toc: false
# 绝对访问路径
# Absolute link for visit
#url: "mysql面经.html"
# 开启文章置顶，数字越小越靠前
# Sticky post set-top in home page and the smaller nubmer will more forward.
#weight: 1

# 开启各种图渲染，如流程图、时序图、类图等
# Enable chart render, such as: flow, sequence, classes etc
#mermaid: true
---

> 📈 mysql面经

本文中主要介绍一些mysql的面试笔记，文章持续更新，面试答案参小林coding的[图解系列](https://xiaolincoding.com/)，主要围绕一条SQL的执行过程穿插了很多面试题，并且按照章节进行介绍

<!--more-->

#### sql语句执行的流程

<img src="https://zzzi-img-1313100942.cos.ap-beijing.myqcloud.com/img/202402280956296.png" alt="查询语句执行流程" style="zoom: 50%;" />

1. 连接器：建立连接（TCP），之后所有的查询操作都会先判断当前用户的权限，长时间（28800）不用的连接会被释放，并且连接数不能超过151

2. 查询缓存：select会查询缓存，看是否曾经执行过，对于更新频繁的表，缓存基本无用

3. 解析SQL：主要是词法分析和语法分析，分析成功就会得到一个SQL**语法树**

4. 执行SQL：

   - 预处理SQL：替换*，判断字段和表是否存在

   - 优化SQL：使得SQL执行的效率变高，例如存在主键索引（id）和普通索引（其余字段加索引）时，会判断使用哪个索引的效率更高，需要的列就是id，此时利用二级索引更快，因为二级索引的叶子结点保存的就是id

   - 执行SQL：与存储引擎交互查询数据

     - 主键索引查询：直接利用主键索引查询

     - 全表扫描：全表扫描一遍

     - 索引下推：将索引的判断工作交给存储引擎端，减少**回表**操作，这里涉及到一个联合索引的操作，由于存在最左匹配原则，当出现范围查询时，联合索引的范围查询可以使用索引，但是剩下的字段无法使用索引（因为局部无序），所以剩下的字段需要进行回表，将存储引擎查询到的数据在服务端再进行筛选，索引下推可以使得这个筛选工作在存储引擎端完成，从而减小回表操作，提高效率，**举例**：

       > `select * from t_table where a > 1 and b = 2`，联合索引（a, b）
       >
       > 此时联合索引会先按照a排序，然后存储引擎找到所有a>1的数据的id，在这些数据里，b没有进行排序，所以联合索引失效，这些数据需要进行回表进一步筛选，索引下推可以将筛选的工作交给存储引擎端，减小数据传输的时间

#### mysql一行记录如何存储

> 数据存储由行，页，区，段组成，每一行数据分为额外信息和真实数据，额外信息包含变长字段的长度以及NULL值列表（二者可能不存在），真实数据包含三个隐藏字段（隐藏自增id，事务id，上一个版本指针）以及真实值

一旦建立数据库中的一张表，那么就会在`/var/lib/mysql/` 中建立一个对应的数据库，内部新增三个文件：

```mysql
db.opt  
表名.frm  
表名.ibd
```

1. db.opt：存储数据库的默认字符集和校验规则
2. 表名.frm ：存储表结构信息
3. 表名.ibd：存储表中的数据，默认存储在这个**独占表空间文件**（.ibd文件）中，但是一个参数可以控制数据存放在**共享表空间文件**（.ibdata1文件）中

mysql中数据存储按照**段，区，页，行**来组织，双向链表中连续的页在物理上也连续，一个区有1mb，区内有64个连续的页，而多个区组成一个

<img src="https://zzzi-img-1313100942.cos.ap-beijing.myqcloud.com/img/202402281055554.png" alt="img" style="zoom:43%;" />

对于一行数据，现在默认采用`dynamic`，这是从`compact`改进得到的

<img src="https://zzzi-img-1313100942.cos.ap-beijing.myqcloud.com/img/202402281055841.png" alt="img" style="zoom:50%;" />

1. 变长字段就存储了行记录中变长字段的**具体长度**，**逆序**存放（读指针指向额外信息和真实数据之间，向左读可以读到额外信息，向右读可以读到真实数据）

2. NULL值使用一个**二进制位**记录，也是逆序存放，一行记录的某一列为NULL，二进制位的对应位置就为1，字段不足八位高位补0

> 上面两位都不是必须的，表结构中有变长或者允许为空时才存在这两个信息

3. 记录头信息中存放当前行记录是否被删除，下一条记录的位置等信息
4. 真实数据存在三个隐藏列：
   - row_id：数据没有主键以及非空的唯一字段时增加这个隐藏的自增id
   - trx_id：事务id，标记当前数据被那些事务操作过，根据这个来判断当前事务应该操作这个数据的哪一个版本
   - roll_pointer：是一个undo log的链，不同的版本提供给不同权限的事务

#### mysql中数据页的格式

> b+树中非叶子节点存储的是id（页中最小行记录的id）和页号，一个非叶子节点存储多个这样的记录，叶子结点就正常存储数据页，用双链表组织：
>
> <img src="https://cdn.xiaolincoding.com//mysql/other/6374409c6c404d446855dc6a694b6d26.png" alt="图片" style="zoom:50%;" />

数据页（默认16kb）中存放的是一行一行的数据，是最小的查询单位，多行数据之间进行分组并建立页目录，页目录中的一个槽指向一个分组中的最大记录，相当于数据页中的行数据还进行了分组，查询时是二分+顺序的方式

数据页之间的组织方式是双向链表，数据页内部的行数据组织方式是按主键顺序单向链表

#### InnoDB的行格式

1. redundant
2. compact（紧凑的行格式）
3. dynamic
4. compressed

#### 创建表时建立的索引

1. 有主键：此时用主键做聚簇索引
2. 没有主键：用一个非空的唯一列做聚簇索引
3. 都没有的话就会生成一个隐式的自增id来做索引，这涉及到行的格式，内部有一个隐式的自增id

#### 聚簇索引和二级索引的区别

1. 聚簇索引：通常是主键索引，或者是非空的唯一字段索引，极端情况为隐式的自增id索引，聚簇索引中叶子结点存放的是**数据**，一般建表时就会创建聚簇索引

2. 二级索引：非主键字段的索引，叶子结点存放的是**主键id**，所以查询到id之后还会进一步利用聚簇索引查询到真实的数据，这一操作叫做回表，如果此时刚好需要的就是主键，此时这个过程就叫做**覆盖索引**

   > 相当于给哪个非主键字段加索引，就会形成一个b+树，叶子结点保存的是主键id，而不是数据，需要进行**回表**

#### 什么叫覆盖索引

> 使用二级索引一次就能查询得到结果的过程

当非主键字段存在索引时，在这个字段上进行查找会最终找到b+树中的一个叶子结点得到一个主键值，正常情况下需要回表得到主键值对应的真实数据，但是如果我们**正好需要的就是这个主键值**，那么就**不需要回表**，此时成为覆盖索引

#### varchar(n)中的n最大取值是多少

保证真实数据+变长字段所需字节+NULL值列表<=65535字节

#### 行溢出怎么处理

一页大小为16kb，也就是16384字节，而一行数据最大为65535字节，如果一页存储不了一行数据，那么多余的数据就会存储在**溢出页**中，存放真实数据的地址中会分配一部分指向溢出页的地址

<img src="https://zzzi-img-1313100942.cos.ap-beijing.myqcloud.com/img/202402281056468.png" alt="img" style="zoom:33%;" />

#### mysql的存储引擎

1. innodb（默认）
2. myisam
3. memory

#### 索引定义及分类

索引就是数据的目录，可以加快查找数据的速度

![image-20240229093806087](https://zzzi-img-1313100942.cos.ap-beijing.myqcloud.com/img/202403041300393.png)

> 给谁建立索引，谁就是b+树节点中的key值

#### 联合索引的最左匹配

由于存在最左匹配，所以要先匹配左边的字段，例如给（a,b,c）建立索引，此时就要先匹配a，因为内部按照a进行排序，其余两个字段是全局无序的（只有a相同才会对b排序，只有a和b相同才会对c排序）

如果不按照最左匹配，联合索引就会失效，存在只有部分字段使用到了联合索引的情况，select * from t_table where **a > 1** and b = 2和select * from t_table where **a >= 1** and b = 2存在区别，后一句中的a可以等于1，于是内部的b可以使用到联合索引，前一句不行

> 联合索引是否失效取决于最左匹配的字段**是否可以相等**

#### 联合索引的区分度

建立联合索引时，区分度大的字段要排在左边

![区分度计算公式](https://zzzi-img-1313100942.cos.ap-beijing.myqcloud.com/img/202402291004066.png)

不同值越多，区分度越大

#### b+树相对于b树，二叉树，哈希的优势

1. b树每个节点都存储值，b+树只有叶子结点存储数据，单个节点的数据量更小，一次i/o读取到的节点更多
2. 二叉树孩子节点只能有两个，b+树可以有多个，树高更小，i/o次数就更少
3. hash只适合做等值查询，不适合做范围查询

#### 索引优化

1. 前缀索引优化：只将一个字段的前几个值建立索引，减小索引字段的大小
2. 覆盖索引优化：将要查询的字段建立一个联合索引，这样就不用回表，使用主键再次查询
3. 主键索引递增：由于b+树中的数据按照顺序存放，主键递增可以使得新增数据是追加操作，不用移动数据
4. 索引设置非空：存在NULL导致索引建立更加复杂，且NULL值还会占用行记录的空间（NULL值列表占用空间）

<img src="https://cdn.xiaolincoding.com/gh/xiaolincoder/mysql/%E7%B4%A2%E5%BC%95/%E7%B4%A2%E5%BC%95%E6%80%BB%E7%BB%93.drawio.png" alt="img" style="zoom:50%;" />

#### 索引失效

1. 对索引字段使用**左**或者**左**右模糊匹配（`like %xx`或者`like %xx%`）：只要左边模糊了，那么索引就会失效，因为索引匹配按照**前缀**，这里左边模糊了，前缀无法匹配，自然失效

   > 这里有特殊情况（**没问到就不说**），当表中的字段没有非索引字段时，索引就不会失效，因为此时二级索引形成的b+树就可以直接得到结果（覆盖索引），比全表扫描更快，这取决于**优化器**的选择

2. 对索引字段使用**函数**：因为索引字段建立索引使用的是原值，而不是使用函数后的值，相当于没有这种索引（`select * from t_user where length(name)=6`）

   > mysql8之后增加了函数索引，此时才可以对索引字段使用函数

3. 对索引字段进行**表达式计算**：表达式计算之后的值没有建立索引，所以索引失效（`select * from t_user where id + 1 = 10`）

4. 索引字段发生**隐式类型转换**：mysql会自动将<u>字符串转换成数字</u>，所以下面的phone都转换成了数字，而没有这种索引，‘1’转换成数字，存在这种索引

   > 主要看字符串转成数字还是数字转换成字符串

   - select * from t_user where phone = 1300000001**失效**
   - select * from t_user where id = '1'**不失效**

5. 联合索引**非最左匹配**：先按照左边的排序，左边的相同才按照右边的排序，所以使用时没有先匹配左边的，联合索引就会失效（对a,b,c建立索引但是匹配b或者c就会索引失效，但是匹配a就不会）

6. 联合索引**索引截断**：对a,b,c建立索引，对于a进行范围查询且**没有等于**，后面的b就不会进行联合索引查询，因为只有a相等才会对b进行联合索引查询

> 在联合索引的情况下，按照索引第一列排序，第一列数据相同时才会按照第二列排序

7. where子句中的or：or的前后只要有一个不是索引，就会进行全表扫描，因为不是索引的字段会进行全表扫描，再进行索引反而浪费
8. 非空查询：这种情况下，全表扫描反而更方便
9. 扫描的行记录过多：此时优化器可能选择全表扫描，就不会走索引了

#### mysql为什么使用b+树做索引 

1. 索引存储在磁盘中（**速度**慢），要尽可能减小磁盘读写次数
2. b+树做索引使得查找和新增索引时都比较快（**效率**高），并且层高较低
3. 只有叶子结点存放真实数据，同样的大小，b+树**存储的节点更多**，一次读写的索引更多
4. b+树中的叶子结点使用双链表连接，**范围查询**很方便

#### mysql单表不要超过2000W

> 根据非叶子节点能存储的其他页（当前页最小数据的id+页号）的数量以及叶子节点能存储的数据行数决定单表最大存储数据量

1. B+树的节点是一个大小为16k的页，去除掉头信息等内容，剩下大约15k存储数据
2. 大约15k能存储1000多个指针，也就是B+树有1000多叉
3. mysql建议B+树不超过3层，计算得到叶子结点大约`1000^(3-1)=1000000`个
4. 每个叶子结点15k存储数据，一条数据按1k算，存15条数据
5. 所以一个B+树大约存储1500W条数据

> 原理就是计算出n叉b+树的叶子结点个数，然后看每个叶子结点能存储多少条数据

#### 关于count(*)和count(1)

<img src="https://cdn.xiaolincoding.com//mysql/other/af711033aa3423330d3a4bc6baeb9532.png" alt="图片" style="zoom:50%;" />

> count函数执行时，server层会维护一个count变量记录结果

1. **count(主键字段)**的执行过程

   1. 只有主键索引（聚簇索引）就利用主键索引查询，判断其id是否为空，从而更新count

   2. 有二级索引就**先用**二级索引，判断其id是否为空从而更新count，因为二级索引中存储的是主键id，不像主键索引中存储的是数据，查询效率更高

      > 因为二级索引叶子结点存储的是主键id，而聚簇索引叶子结点存储真实数据，相同数量的索引，二级索引占用空间更小，i/o成本更小

2. **count(1)**的执行过程

   1. 只有主键索引（聚簇索引）就利用主键索引查询，直接更新count

   2. 有二级索引就**先用**二级索引，直接更新count，因为二级索引中存储的是主键id，不像主键索引中存储的是数据，查询效率更高

      > 相比于count(主键字段),count(1)不用判断id是否为空，**效率更高**

3. **count(*)**执行过程

   > 将*转换为0，剩下过程与count(1)一样
   >
   > MyISAM中有一个变量记录了没有条件筛选的count（*）的结果

4. **count(字段)**的执行过程

   > 此时这个字段没有索引，只能全表扫描，效率最差，并且不统计NULL

#### 优化count(*)

1. 不直接执行count(*)，而是使用效率更高的`explain`得到近似值

   > 适用于不需要精确结果的统计

2. 将count数目单独维护到额外的一张表中，增删都更新这张表

#### 事务的特性（ACID）

1. 原子性（A）：事务就是最小的执行单位，事务中的所有操作要么全部完成，要么全部不完成
2. 一致性（C）：事务执行前后，数据保持一致，例如转账的总金额保持一致，出多少入多少
3. 隔离性（I）：事务之间的执行是隔离的
4. 持久性（D）：事务的操作被提交之后，影响是永久的

<img src="https://zzzi-img-1313100942.cos.ap-beijing.myqcloud.com/img/202312111423205.png" alt="AID- style="zoom: 33%;" >C" />

#### 事务的传播行为

> 当事务方法被另一个事务方法调用时，必须指定事务应该如何传播

|            传播行为            |                             说明                             |
| :----------------------------: | :----------------------------------------------------------: |
|   **`PROPAGATION_REQUIRED`**   | 当前不存在事务，那么就新建一个事务，否则就加入当前的事务，这是默认使用的事务传播行为。如果方法 A（PROPAGATION_REQUIRED）调用了方法 B（PROPAGATION_REQUIRED），方法 B 将加入到方法 A 的事务中。如果方法 B 抛出异常，则方法 A 和 B 都会回滚。 |
| **`PROPAGATION_REQUIRES_NEW`** | 当前存在事务，会将当前事务**挂起**，然后执行自己的事务，相当于事务之间互不影响，挂起当前事务的操作是与**`PROPAGATION_NESTED`**最核心的区别 |
|    **`PROPAGATION_NESTED`**    | 当前存在事务，会在当前事务内部重新创建一个事务，二者相互不影响，并且当前事务并不会被挂起，如果当前不存在事务，那么**`PROPAGATION_NESTED`**与**`PROPAGATION_REQUIRED`**等价 |
|    `PROPAGATION_MANDATORY`     |         当前存在事务加入当前事务，不存在事务抛出异常         |
|      `PROPAGATION_NEVER`       |            以非事务方式运行，当前存在事务抛出异常            |

> 事务的传播行为指的是事务在执行过程中**遇到其他类型的事务怎么处理**，分为加入，新建，嵌套，报错等处理方式

#### 事务的数据一致性问题

在介绍事务的隔离级别之前，会介绍并发事务中可能遇到的一些数据一致性问题：

1. **脏读**（读未提交）：指的是一个事务读取了另外一个事务未提交的数据。一个事务由于一些情况修改了数据但是最终并没有提交，而这个没有被提交的事务被其他事务读取到就会出现脏读
2. **不可重复读**（数据值不一样）：指的是对于**同一行**数据来说，由于没有针对事务的并发操作进行控制，事务的多次读取出现了**数据不一致**的问题（数据值）
3. **幻读**（数据量不一样）：指的是**范围操作**中，多次范围操作的**数据量**不一致，好像出现了幻觉，多读取或者少读取了一些数据（数据量）

#### 事务的隔离级别

|                           隔离级别                           |                             说明                             |
| :----------------------------------------------------------: | :----------------------------------------------------------: |
|            **`ISOLATION_DEFAULT`** <br />（默认）            | 后端使用什么数据库，就采用什么隔离级别，MySQL 默认采用的 `REPEATABLE_READ` 隔离级别， Oracle 默认采用的 `READ_COMMITTED` 隔离级别 |
|      **`ISOLATION_READ_UNCOMMITTED`**<br />（读未提交）      |   允许读取未提交的数据会出现脏读，幻读，不可重复读三种问题   |
| **`ISOLATION_READ_COMMITTED`** <br />（读已提交`ReadView`）  | 只允许读取已提交的数据，但是对于这一行数据没有进行并发控制，不会出现脏读，但是幻读和不可重复读还是会出现 |
| **`ISOLATION_REPEATABLE_READ`** <br />（可重复读`ReadView`） | 允许重复读，给当前行数据增加了并发控制，使得多次读取的数据是一致的，但是对于范围操作并没有进行并发控制，所以会出现记录数的变化，幻读还是会出现 |
|        **`ISOLATION_SERIALIZABLE`** <br />（序列化）         | **加读写锁**，事务之间操作数据的顺序是序列化的，但是这种隔离级别大大降低了数据库的性能 |

> 默认情况下，mysql采用可重复读来避免脏读和不可重复读，幻读**很大程度上被避免**

#### 如何尽可能避免幻读

> InnoDB**默认**的隔离级别是RR（可重复读），可以解决脏读和不可重复读，只解决了快照读情况下的幻读问题,当前读情况下解决幻读问题得靠**next-key lock**。<u>无法完全解决</u>

1. 针对快照读(读旧版本记录)：通过**MVCC**（对于读提交，快照（ReadView）会在每个语句中创建。对于可重复读，快照是在事务启动时创建的）的方式，在事务读取数据的过程中如果插入了一条数据，那么此时是读取不到刚刚新增的数据的

2. 针对当前读（读最新版本记录）：通过**next-key lock**（间隙锁+记录锁） 技术，当执行语句时，会加上next-key lock，当其他事务在这个范围内插入或者修改数据则会阻塞，无法成功插入

   > 记录锁就是针对正在操作的数据加锁，间隙锁就是正在操作的记录中间的间隙（正在操作4和6，那么其余的事务不允许操作5）不允许立马操作

#### mvcc中的readview

> readview控制权限，让不同的事务访问不同版本链中的数据，这种方式称为mvcc多版本控制

1. mvcc称为多版本控制。当前事务针对当前版本的数据没有访问权限，那么就会根据版本链找到旧版本
2. 版本链保存在当前行记录的上一个版本指针中，每个数据都有上一个版本，从而形成版本链
3. 数据的上一个版本其实指向的是一个undolog日志，这里保存了旧版本的记录
4. 判断当前事务是否有权限操作当前数据是根据readview来控制的
5. readview中有几个字段来将事务id分类，分为readview创建之前的提交的事务id，readview创建之后的提交的事务id，以及还未提交的事务id
6. 针对当前事务所属的范围来判断当前事务针对当前版本的数据是否可见
7. 在可重复读下，一个事务创建一个readview，这样只能操作当前这个快照下的版本，新版本的数据无法访问，所以可重复读并且避免了幻读
8. 在读提交下，一个事务内针对每次读创建readview，两次读到的数据虽然都是已提交的，但是两次读之间可能出现数据的修改，所以不能保证可重复
9. 一旦有了readview，数据被增删对当前事务来说不可见，自然避免了幻读

#### readview介绍

ReadView有四个字段：

可以理解为一个快照，从而使得读已提交和可重复读这两种隔离级别可以正常运行，ReadView有四个字段：

<img src="https://cdn.xiaolincoding.com/gh/xiaolincoder/ImageHost4@main/mysql/%E4%BA%8B%E5%8A%A1%E9%9A%94%E7%A6%BB/readview%E7%BB%93%E6%9E%84.drawio.png" alt="img" style="zoom:50%;" />

所以一旦有了ReadView之后，一个记录涉及到的事务id就可以被分为三种：

<img src="https://cdn.xiaolincoding.com/gh/xiaolincoder/ImageHost4@main/mysql/%E4%BA%8B%E5%8A%A1%E9%9A%94%E7%A6%BB/ReadView.drawio.png" alt="img" style="zoom:50%;" />

当一个事务去操作记录时，有三种情况：

1. 如果记录中已提交事务的id小于当前事务的id，那么意味着针对当前版本的记录来说，当前事务已提交，可以直接访问
2. 如果在没有开始的事务中，那么意味着当前版本的记录在当前事务创建后产生，不可访问
3. 如果在已启动但未提交中，那么意味着当前版本的记录来说，当前事务没提交，不能访问

#### 可重复读的工作原理

> 始终使用一个ReadView，不更新内部事务id的状态，每次读的都是同一个版本

在事务启动前开启一个事务并生成一个ReadView，之后全程使用这一个ReadView，这样就可以实现多次读取记录是一样的。按照ReadView所记录的事务id来判断当前记录是否可以读取，如果不可以读取就根据记录中的上一个版本指针找到上一版本，就这样根据ReadView判断当前记录是否可读，最终能找到可以读的记录，由于ReadView始终一样，所以可以重复读

#### 读已提交的工作原理

> 每次创建新的ReadView，从而更新事务id的状态，每次读的都是新提交的

每次读取都会生成一个ReadView，ReadView更新之后，就可以读到最新提交的版本。具体还是按照里面记录的事务id判断当前版本的记录是否可以访问，不可以的话就按照上一版本链最终找到可以读取的版本，ReadView一更新就可以读取到最新的版本数据

> 区别就是每次操作是否新建一个ReadView，新建了ReadView就会更新事务id的状态，是活跃还是已提交，从而实现可重复读和读已提交
>
> **可重复读**从头到尾都只有一个ReadView，从而每次读取到的都是事务开启时能够读取到的版本
>
> **读已提交**每次更新ReadView，从而内部的事务id更新，导致每次读取到的都是最新提交的版本

#### 可重复读不能完全解决幻读

- 针对**快照读**（普通 select 语句），是通过 MVCC 方式解决了幻读。

- 针对**当前读**（select ... for update 等语句），是通过 next-key lock（记录锁+间隙锁）方式解决了幻读。

  > 1. 针对快照读：当事务 A 更新了一条事务 B 插入的记录，那么事务 A 前后两次查询的记录条目就不一样了，因为权限遍历
  > 2. 针对当前读：事务开启之后先进行快照读，读取到的是旧数据，然后别的事务插入数据之后在进行当前读（要读取最新的），此时前后两次读取的记录数不一样
  >
  > 这两种情况都很**极端**：
  >
  > 1. 一般修改时不会针对快照读，而是针对当前读
  > 2. 一般针对当前读内部不会出现快照读
  >
  > 但是也说明了可重复读无法完全解决幻读

#### 针对当前读解决幻读的记录锁和间隙锁

1. 记录锁就是将当前正在操作的记录锁起来，使其他事务不能访问：防止当前记录数减小
2. 间隙锁就是将当前正在操作的记录之间的间隙锁起来，使得不能在其中增加记录：防止记录数增加

#### mysql有哪些锁

> 普通的select不用加锁，直接使用mvcc就可以实现一致性
>
> 需要更新的语句才需要加锁

1. 全局锁：一旦开启，整个数据库只读，影响效率，适用于数据库备份，可以在备份开始时加一个ReadView（备份的是开启ReadView时的版本），这样就可以提高备份的效率，因为别的事务还可以处理数据，当前备份的不是被新修改的版本

2. 表级锁

   1. 表锁：锁整张表，相比全局锁更加细粒度，有共享锁和独占锁，共享锁可以读，尽量避免加表锁，因为有更加细粒度的行级锁
   2. 元数据锁：防止增删改查时**表结构**变化，分为读锁和写锁：
      - 增删改查时加读锁，防止别的线程改变表结构
      - 修改表结构时加写锁防止冲突
   3. 意向锁：给记录加锁时，会给表增加一个意向锁，此时后来还想加锁，看到表上有意向锁，就意味着有记录已经被锁住了，不用依次遍历去判断，用来快速判断表中是否有记录被枷锁
   4. 自增锁：插入数据时增加自增锁，保证记录的**自增id唯一**，开始是插入完成才释放锁，之后是一旦分配一个id不等插入完成就释放

3. 行级锁

   1. 记录锁：分为共享锁（S锁）和排他锁（X锁）：排他锁只能存在一个

      <img src="https://cdn.xiaolincoding.com/gh/xiaolincoder/mysql/%E9%94%81/x%E9%94%81%E5%92%8Cs%E9%94%81.png" alt="img" style="zoom:33%;" />

   2. 间隙锁：为了解决幻读问题，如果对id**范围(**3,5)加锁，那么就不能插入id为4的记录

   3. next-key lock：记录锁+间隙锁的组合，可以解决**当前读**下的幻读问题

   4. 插入意向锁：插入记录时，插入位置有间隙锁，此时会生成一个插入意向锁，代表自己想插入，但是还没成功

#### 元数据锁出现的问题：

1. 事务A查询数据的过程中加上了元数据读锁，其他想要读数据的事务不阻塞
2. 事务B想要修改表结构，由于存在元数据读锁，所以被阻塞
3. 由于写锁的优先级更高，后续想要读的事务加读锁会被阻塞，也就是后续的读请求无法响应
4. 这种无法响应的读请求过多就会出现问题

#### 自增锁在主从复制中出现数据不一致

**出现的问题：**

在主库中有两个连接都在插入数据，A插入了id为1和2的数据，之后B插入了id为3的数据，最后A再插入了id为4的数据，此时A插入的数据id不连续

在主从复制的过程中，从库按照顺序执行语句，如果日志中记录的是sql语句（statement），那么就就会先执行A的三条数据，再执行B的一条数据，导致A的三条数据id为1,2,3，B的数据id为4，导致数据不一致

**解决办法：**

解决办法就是主从复制记录日志时，记录的不再是sql语句statement，而是直接保存行记录（row），这样主库中的数据是什么id，从库中就是什么id，不会出现数据不一致的问题

#### mysql行级锁的退化（如何加行级锁）

行加锁主要是为了防止幻读现象，其余的数据不一致完全可以通过设置事务隔离级别实现

> 不同的查询会加不同的锁，但是这些锁基本上都是加在了索引上，并且加锁的基本单位就是next-key lock，只是部分情况下会**退化**
>
> 能用记录锁或者间隙锁就可以解决幻读的情况时，锁就会退化，并且相等的记录一般加记录锁，不相同的记录一般加间隙锁

1. 唯一索引等值查询：
   - 查询的记录存在：next-key lock退化成记录锁就可以解决幻读
   - 查询的记录不存在：第一条大于的记录的next-key lock退化成间隙锁，当前记录不存在，无法加记录锁，间隙锁的范围是第一条大于的记录以及这条记录的前一条记录
2. 唯一索引范围查询：
   - 大于等于或小于等于：针对等于的这条记录，next-key lock退化成记录锁就可以解决幻读，剩下的记录加next-key lock即可
3. 非唯一索引等值查询：
   - 记录存在：等值的记录（可能多个）加的是next-key lock，对他们的主键加记录锁，第一个不符合条件的加间隙锁，
   - 记录不存在：第一个不满足的加间隙锁，不存在等值记录，主键索引不加锁
4. 非唯一索引范围查询：都加的是next-key lock
5. 没有加索引的查询：都加的是next-key lock，这可能会出现锁全表的情况

> 单纯的间隙锁之间是兼容的，他们只是为了保证间隙之间不会被插入数据

#### update没加索引会锁全表？

> 锁全表不是加了表锁，而是因为update回家next-keylock，没加索引会全表扫描，每一条记录都加的是next-key lock

**为什么？：**

1. 因为update时，加锁的基本单位是next-key lock，而where条件没有带上索引列，导致进行了全表扫描，从而使得全表的记录都加上了next-key lock，这**相当于**将全表锁住
2. 即使带上了索引列，如果查询时优化器选择了全表扫描，那么全表还是会被锁住

**解决办法：**

> 开启mysql的**安全更新**模式（有一个参数），此时update和delete**满足一定条件才能执行成功**，从而从语法层面避免所有的记录都加上了next-key lock

1. update 语句必须满足如下条件之一才能执行成功：

- 使用 where，并且 where 条件中必须有索引列，如果带上了索引列还是使用了全表扫描，此时可以手动强制指定优化器使用索引
- 使用 limit；
- 同时使用 where 和 limit，此时 where 条件中可以没有索引列；

2. delete 语句必须满足以下条件能执行成功：

- 同时使用 where 和 limit，此时 where 条件中可以没有索引列；

3. 强制指定使用索引，避免优化器自动选择扫描全表

#### mysql中死锁的情况

> 由于事务之间彼此通信而造成的一种阻塞的情况，事务之间都无法继续执行

1. A访问表a，B访问表b，都加了表锁，之后A访问表b，B访问表a，由于都有表锁，都等着对方释放表锁，导致出现死锁
2. sql语句中的筛选条件没有带上索引列引发了全表扫描从而全表记录锁定，此时如果有其他事务进来想访问一些记录就可能出现死锁
3. A查询一条数据之后尝试更新，B尝试更新，A查询时的共享锁没有释放导致B的排他锁无法加上，也就是B无法完成更新，而A要更新要等B的排他锁释放，互相等待出现死锁

#### 如何避免死锁

> 死锁产生有四个必要条件，只要打破这四个条件之一就可以解决死锁：
>
> 1. 互斥：一个时间段内的资源只能由一个进程使用
> 2. 请求和保持：进程已经有了一定的资源但是还要请求新资源
> 3. 不剥夺：没使用完就不能强行剥夺
> 4. 环路等待：发送死锁时，必定会出现进程和资源的环路等待

1. 设置事务的等待超时时间：事务等待超过一段时间就认为其发生了死锁，对这个事务回滚
2. 主动监测：检测到死锁之后，将环路等待中的某个事务回滚，打破这种环路等待
3. 数据检索尽量都带上索引列，从而避免锁全表记录导致的死锁
4. 减少事务一次访问的资源数量

#### mysql中的日志种类

1. undo log：回滚日志，用于保证事务的**原子性**，要么全部成功，要么全部失败（回滚）
2. redo log：重做日志，保证事务的**持久性**，出现断电等故障时用于恢复事务
3. binlog：归档日志，用于数据备份和主从复制

#### undo log回滚日志

> 主要在增删改时自动开启事务并记录undo log，回滚时做反操作

1. 插入数据：记录当前数据的id，回滚时按照id删除
2. 删除数据：记录当前数据的内容，回滚时按照内容新增
3. 修改数据：记录当前数据的旧值，回滚时按照旧值恢复

#### undolog的作用

> 是写前日志，在操作之前就先记录日志，redis中的aof日志文件时写后日志，只记录成功的日志

1. 实现MVCC，readview会根据版本链指针找到有权限查看的undolog版本拿到对应的数据
2. 实现事务的原子性，事务中的操作一旦失败，就会使用undolog来进行回滚，是写前日志

#### 为什么需要BufferPool

提高数据的查询速度，Pool中是以数据页（16kb）为基本单位的

1. mysql启动时申请的，默认为128MB

2. 会将要操作的页放入BufferPool中加快操作速度

3. 在BufferPool中存在一些缓存页，缓存页有多种（**数据页，索引页，插入缓存页，undo页，锁信息**），这些页在缓存中的组织顺序为：

   ![img](https://cdn.xiaolincoding.com/gh/xiaolincoder/ImageHost4@main/mysql/innodb/%E7%BC%93%E5%AD%98%E9%A1%B5.drawio.png)

   会存在一些碎片空间无法被利用

4. 缓存时以页为单位，而不是以记录，然后从页中找到指定的记录
5. 为了快速找到BufferPool中的空闲空间，定义了`free`链表，从中就可以得到空闲空间
6. 为了快速找到脏页（已更新但是没有持久化的页），定义了一个`flush`链表
7. 为了提交缓存命中率从而加快效率，此时使用了LRU（最近最少使用）算法，也就是缓存空间不够时，先淘汰最近最少使用的页

#### BufferPool如何防止预读失效

> 根据空间局部性原理，当前访问的数据附近的数据也有一定概率读取，所以会将这些数据读取到缓存中，但是之后没有用到，就出现了预读失效

1. 将缓存中的页分成old和young两个链表
2. 预读的页放到old的头部
3. 有页面被访问，将其放到young的头部，young尾部的数据降级到old的头部，此时没有数据淘汰
4. 淘汰页面是先淘汰old的尾部

#### BufferPoll污染

> 由于类似于全表扫描这种大批量的扫描记录导致BufferPoll中的所有缓存数据更新，经常访问的数据不在缓存中，导致后期访问出现大量的磁盘i/o

1. 提高进入young区域的门槛
2. 当一条记录在old区域被第一次访问之后在**一段时间**内再次被访问，此时就不会将其加入young区域
3. 如果这条记录的两次访问时间不在一段时间间隔内，此时将其加入young中

这样就可以使得BufferPoll不被污染

#### 脏页持久化的时机

1. redolog记录满了，此时需要删除redolog，就需要将对应的脏页持久化，使得被删除的redolog失效，才能正常删除
2. BufferPoll满了之后，会删除一部分脏页，此时就会先持久化再删除
3. mysql认为当前空闲，会抽空将脏页持久化
4. mysql正常关闭之前，会将脏页持久化

> 在脏页持久化时可能会偶尔造成某些SQL语句的执行时间出现异常变长的情况，这就是因为脏页持久化导致时间变长

#### redo log重做日志

> 为了保证事务的持久性，记录的是更新之后的值

1. 更新数据更新的是缓存中的数据页，该数据页先被标记为脏页
2. 记录redo log并持久化到磁盘中
3. 在一段时间过后，mysql统一将所有的脏页持久化到磁盘中
4. 如果这段时间内缓存中的数据丢失，也可以通过磁盘中的redo log恢复
5. 即使更新的是缓存中的undo log信息，其也会记录对应的redo log

#### redolog的作用

1. 能够保持数据的持久性，即使mysql发生宕机，只要redolog还在，那么就可以恢复数据
2. 有了redolog，真实数据不用每时每刻都持久化到磁盘，**减少随机写的次数**，同步redolog使用追加的方式，也就是顺序写

#### redo log如何缓存

> 更新记录时产生的redo log还是先缓存，并不会立马持久化到磁盘

1. mysql正常关闭，此时redo log持久化到磁盘
2. 存储redo log的专用缓存空间被占了一半以上，此时会持久化
3. 每隔一秒自动持久化
4. 每次事务提交自动持久化（可以用参数控制）
   1. 参数为0：事务提交，redo log的缓存不主动持久化
   2. 参数为1：事务提交将redo log的缓存持久化到磁盘
   3. 参数为：事务提交将redo log的缓存先保存到文件系统中的缓存中，之后再持久化

#### redo log文件写满了

> 循环写+写不下就开始持久化脏页擦除失效的redo log

1. innoDB中的redo log文件有**两个**，并且是循环写的，第一个文件写满了写第二个，第二个写满了又从第一个开始写

2. 一旦脏页被持久化到了磁盘中，针对这些数据的redo log就没用了，就可以擦除

3. 如果某一次写入的redo log写不下了，就开始擦除无用的redo log

4. 没有无用的redo log，就将脏页持久化一部分，这样这些数据的redo log就可以擦除从而腾出空间了

#### binlog归档日志

> 在server层记录所有的表结构和数据变化的日志，不记录**查询类**的日志

#### redolog和undolog的区别

1. redolog记录的时操作之后的值，undolog记录的是操作之前的值
2. redolog是重做日志，为了保证mysql中的数据持久性
3. undolog是回滚日志，负责事务的回滚
4. 事务提交之前出现错误，使用undolog进行回滚
5. 事务提交之后出现错误，使用redolog进行恢复

#### binlog和redo log的区别

1. binlog是server层实现的，redo log是存储引擎端实现的
2. 文件格式不同：binlog可以记录statement（sql语句），row（真实数据），mixed（混合前面两种），redo log是物理日志，直接记录哪个位置的哪个数据发生了什么变化
3. binlog是**追加写**，写不下会新增一个文件，redo log是**循环写**，写不下就从头开始写，并且还可能出现覆盖数据的情况
4. 用途不同：binlog用于数据备份主从复制，redo log用于故障恢复
5. 与mysql的发展有关，最开始mysql中没有redolog，无法实现保证部分缓存数据的持久性，也就是未持久化的脏数据不会丢
6. 后期mysql以插件形式引入innodb之后，增加了redolog来实现crash-safe功能

#### 整个数据库数据都删除，使用什么日志恢复

要使用binlog而不是redo log，因为redo log是循环写，只记录当前脏页的redolog，binlog是追加写，记录所有的日志

#### 主从复制实现原理

> 经历下面三步就可以实现在主库更新，从库也可以变化，这样主库中由于一些操作导致记录加了锁也不会影响从从库中读取数据

1. 写入binlog：主库在运行过程中产生binlog
2. 同步binlog：从库将主库中的binlog复制过来
3. 回放binlog：从库根据复制来的binlog来同步数据

#### binlog什么时候持久化

> 每次事务提交时进行持久化，有一个参数控制持久化到哪

1. 参数为0，每次事务提交直接将binlog存储到文件系统的缓存中，由os决定什么时候持久化到磁盘
2. 参数为1，每次事务提交都将binlog存储到文件系统的缓存中，然后立马持久化
3. 参数为N：每次事务提交都将binlog存储到文件系统的缓存中，积累N个事务的binlog后就持久化

参数为0安全性不高，参数为1效率不高

#### 更新操作的执行流程

> 一旦修改数据，就会记录undolog，一旦修改任何内容（包括日志），就会记录redolog，binlog是为了主从复制，三种日志的作用不一样

1. 前期准备得到执行计划
2. 得到要更新的记录所在的页（不在缓存中要从磁盘中读取到缓存中）
3. 将更新的信息传递给存储引擎开始更新
4. 记录undo log，此时会修改缓存中缓存的undo log，要将其对应的redo log记录一下
5. 相当于undolog本身也需要记录一个redolog
6. 更新记录，标记当前记录所在页为脏页，并且记录针对当前数据的redo log
7. 更新完成之后，记录对应的binlog
8. 事务提交（涉及到两阶段提交）
9. 相当于涉及到两种redolog的记录，一个针对undolog本身，一个针对数据本身

#### 为什么有事务的两阶段提交

为了防止binlog和redolog日志记录**半成功**导致的主从不一致问题，例如：

1. 只成功记录redo log，此时崩溃之后进行恢复，主库中的数据是最新的，但是binlog没成功记录，从库中根据不是最新的binlog进行主从复制，得到的是**旧值**
2. 只成功记录binlog，此时崩溃之后进行恢复，主库中的数据是旧的，从库根据最新的binlog进行主从复制，得到的是**新值**

#### 两阶段提交的过程

> 将redolog的写入拆成两部分，prepare和commit，然后在其中插入binlog的写入
>
> 提交成功的标志是binlog是否成功写入，**记录事务id是很重要的**

1. prepare：将事务id写入redolog，并且将其状态设置为prepare，之后将其持久化到磁盘
2. commit：将事务id写入binlog，并且持久化到磁盘，之后将redolog的状态设置为commit

#### 两阶段提交中发生异常重启

> 主要是看日志之间是否存在逻辑不一致的问题导致数据不一致

重启之后如果redolog的状态为commit就没有出现问题，**如果不是commit**，此时拿着redolog中记录的事务id查看binlog中是否存在这个事务id，存在两种情况：

1. binlog中没有redolog中记录的事务id，说明binlog没有成功记录，两份日志的记录的东西不一样，此时根据redolog回滚

2. binlog中有redolog中记录的事务id，说明binlog成功记录，只是忘记更改状态，此时直接提交

   > 并且**必须提交**，因为从库中已经拿着这个binlog进行了主从复制，如果主库此时回滚，就会出现问题

#### 两阶段提交存在的问题

1. 效率不高，因为磁盘i/o高

   > 为了解决磁盘i/o高，此时可以将binlog的持久化次数减小，多个binlog才完成一次持久化，但是要将binlog写入文件系统的缓存中，并且可以通过设置他们的参数来控制持久化的时机

2. 两阶段提交需要加锁保证原子性，也影响效率

 
